# -*- coding: utf-8 -*-
"""Walmart paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19GEXni1eRtqgKDLI16Mzibvpi55dk4zl
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime
import calendar
import math
# Importing the regression libraries
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from pandas.util._decorators import Appender
#from pmdarima.arima import auto_arima
from statsmodels.tsa.arima_model import ARIMA
from sklearn.tree import DecisionTreeRegressor

#Loading the data from csv files.
train=pd.read_csv('train.csv')
features=pd.read_csv('features.csv')
stores = pd.read_csv('stores.csv')

data = train.merge(features, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')
print(data.shape)

print(data)

#We came to know that Type A stores have their medians higher than any other medians in other store types,
#so the weekly sales for store type A is more than other store types.
sorted_type = stores.groupby('Type')
plt.style.use('ggplot')
labels=['A store','B store','C store']
sizes=sorted_type.describe()['Size'].round(1)
sizes

sizes=[(22/(17+6+22))*100,(17/(17+6+22))*100,(6/(17+6+22))*100] # convert to the proportion
fig, axes = plt.subplots(1,1, figsize=(10,10))
wprops={'edgecolor':'black',
      'linewidth':2}
tprops = {'fontsize':30}
axes.pie(sizes,
        labels=labels,
        explode=(0.05,0.05,0.05),
        autopct='%1.1f%%',
        pctdistance=0.6,
        labeldistance=1.2,
        wedgeprops=wprops,
        textprops=tprops,
        radius=0.8,
        center=(0.4,0.4))
plt.show()

data.describe

data.dtypes

master_df=data

master_df=master_df.fillna(0)
master_df.isna().sum()
master_df = master_df[master_df['Weekly_Sales'] >= 0]
# Cleaning holiday columns
master_df['IsHoliday'] = master_df['IsHoliday_x']
master_df = master_df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])

master_df

master_df['Date'] = pd.to_datetime(master_df['Date'], format='%Y-%m-%d')
master_df['Week_Number'] = master_df['Date'].dt.week
master_df['Quarter'] = master_df['Date'].dt.quarter
master_df['Month'] = master_df['Date'].dt.month.apply(lambda x: calendar.month_abbr[x])
master_df["Year"] = master_df["Date"].dt.year

master_df

master_df.dtypes

master_df.isna().sum()

holiday = master_df['Weekly_Sales'].loc[master_df['IsHoliday']== True] # Weekly Sales in Holidays
non_holiday = master_df['Weekly_Sales'].loc[master_df['IsHoliday']== False] #Weekly Sales in Non-holidays.
sns.barplot(x='IsHoliday', y='Weekly_Sales', data=master_df)

data_11= pd.concat([master_df['Dept'], master_df['Weekly_Sales'], master_df['IsHoliday']], axis=1)
plt.figure(figsize=(23,7))
plt.title('Box Plot of Weekly Sales by Department and Holiday')
fig = sns.boxplot(x='Dept', y='Weekly_Sales',showfliers=False, data=data_11, hue="IsHoliday")

data_12=pd.concat([master_df['Weekly_Sales'],master_df['Month'],master_df['IsHoliday']],axis=1)
plt.figure(figsize=(23,7))
plt.title('Box Plot of Weekly Sales by Month and Holiday')
fig=sns.boxplot(x='Month',y='Weekly_Sales',data=data_12,showfliers=False,hue='IsHoliday')

plt.figure(figsize=(17,8))
sns.heatmap(master_df.corr('spearman'), annot = True)

master_df2=master_df.copy()

"""# Pre-processing"""

#Joining the train data with store and features data using inner join.
train = train.merge(features, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')
print(train.shape)

# Make one IsHoliday column instead of two.
train = train.drop(['IsHoliday_y'], axis=1)
train = train.rename(columns={'IsHoliday_x':'IsHoliday'})

# Converting Date to datetime
train['Date'] = pd.to_datetime(train['Date'])

# Extract date features
train['Date_dayofweek'] =train['Date'].dt.dayofweek
train['Date_month'] =train['Date'].dt.month
train['Date_year'] =train['Date'].dt.year
train['Date_day'] =train['Date'].dt.day

train_data1 = [train]

# Converting into numerical variables for the categorical variable 'Type' : A,B,C
type_mapping = {"A": 1, "B": 2, "C": 3}
for dataset in train_data1:
    dataset['Type'] = dataset['Type'].map(type_mapping)

# Converting into numerical variables for the categorical variable 'IsHoliday' : False,True
type_mapping = {False: 0, True: 1}
for dataset in train_data1:
    dataset['IsHoliday'] = dataset['IsHoliday'].map(type_mapping)

# Kaggle has provided some dates to be allocated to special holidays. We have taken the special holidays into account and marked them as holidays.
train['Super_Bowl'] = np.where((train['Date'] == datetime(2010,2,10)) | (train['Date'] == datetime(2011,2,11)) |
                               (train['Date'] == datetime(2012,2,10)) | (train['Date'] == datetime(2013,2,8)), 1, 0)
train['Labor_day'] = np.where((train['Date'] == datetime(2010,9,10)) | (train['Date'] == datetime(2011,9,9)) |
                              (train['Date'] == datetime(2012,9,7)) | (train['Date'] == datetime(2013,9,6)), 1, 0)
train['Thanksgiving'] = np.where((train['Date']==datetime(2010, 11, 26)) | (train['Date']==datetime(2011, 11, 25)) |
                                 (train['Date']==datetime(2012, 11, 23)) | (train['Date']==datetime(2013, 11, 29)),1,0)
train['Christmas'] = np.where((train['Date']==datetime(2010, 12, 31)) | (train['Date']==datetime(2011, 12, 30)) |
                              (train['Date']==datetime(2012, 12, 28)) | (train['Date']==datetime(2013, 12, 27)),1,0)

print('Train holidays:\n')
print ('Christmas Counts:\n', train.Christmas.value_counts(),'\n')
print ('Super Bowl Counts:\n', train.Super_Bowl.value_counts(),'\n')
print ('Thanksgiving Counts:\n', train.Thanksgiving.value_counts(),'\n')
print ('Labor Day Counts:\n', train.Labor_day.value_counts(),'\n')

train

# Since we already have the IsHoliday, these new column variables are redunadant, so we will drop them
dp = ['Super_Bowl','Labor_day','Thanksgiving','Christmas']
train.drop(dp, axis=1, inplace=True)

train

train = train.fillna(0)
#We are removing negative values just in case there is some error in the reporting of weekly sales
train = train[train['Weekly_Sales'] >= 0]
train.shape

# We will also be removing irrelevant columns
features_drop=['Unemployment','CPI','MarkDown5']
train=train.drop(features_drop, axis=1)

print('Final train shape:', train.shape)
train.head(2)

train = train.sort_values(by='Date', ascending=True) # Sorting the data in increasing order of Date and then splitting.
y = train['Weekly_Sales']
X = train.drop(['Weekly_Sales'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Train:Test = 70:30 splitting.
X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.3) #Train:CV = 70:30 splitting.

# Remove Date column as it does not allow the models to fit on the data.
X_train = X_train.drop(['Date'], axis=1)
X_cv = X_cv.drop(['Date'], axis=1)
X_test = X_test.drop(['Date'], axis=1)

# Final shapes.
print('Train:', X_train.shape, y_train.shape)
print('CV:', X_cv.shape, y_cv.shape)
print('Test', X_test.shape, y_test.shape)

"""Define Performance metric - Weighted Mean Absolute Error (WMAE)"""

def wmae_train(test, pred): # WMAE for train
  weights = X_train['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
  error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
  return error

def wmae_cv(test, pred): # WMAE for CV
  weights = X_cv['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
  error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
  return error

def wmae_test(test, pred): # WMAE for test
  weights = X_test['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
  error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
  return error

"""# ARIMA"""

# It is necessary to have the date columns present in the dataset to be available in the datetime format as in ARIMA model it is required to see the sales values in date-wise fashion.
# Hence converting the string formatted Date into datetime format.
master_df.Date = pd.to_datetime(master_df.Date,format='%Y-%m-%d')
master_df.index = master_df.Date
master_df = master_df.drop('Date', axis=1)

master_df = master_df.resample('MS').mean() # Resmapling the time series data with month starting first.

# -Test splitting of time series data
train_data = master_df[:int(0.8*(len(master_df)))]
test_data = master_df[int(0.8*(len(master_df))):]

print('Train:', train_data.shape)
print('Test:', test_data.shape)

# ARIMA takes univariate data.
train_data = train_data['Weekly_Sales']
test_data = test_data['Weekly_Sales']

# Plot of Weekly_Sales with respect to years in train and test.
train_data.plot(figsize=(20,8), title= 'Weekly_Sales', fontsize=14)
test_data.plot(figsize=(20,8), title= 'Weekly_Sales', fontsize=14)
plt.show()

#[Not part of ARIMA] Decomposition of time series data. It is necessary to see whether the trend, seasonality and residual are present in data or not.
from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(master_df['Weekly_Sales'], model='additive')
result.plot()
plt.show()

"""Observation:

1) We can see that trend of weekly sales is decreasing till March and then increasing again.


2) We already know that data contains seasonality around holidays which is corraborated by the symmetric seasonal graph
"""

from statsmodels.tsa.stattools import adfuller
from numpy import log
result = adfuller(master_df['Weekly_Sales'])
print('ADF Statistic: {}'.format(result[0]))
print('p-value: {}'.format(result[1]))
print('Critical Values:')
for key, value in result[4].items():
    print('\t{}: {}'.format(key, value))

"""Observation:

1) Since the p-value is less than 0.05, we can reject the null hypothesis and declare the time series to be stationary.

2) Additionally, since the ADF Statistic is more negative than even the 1% critical value, we can further verify our decision of rejecting the null hypothesis.
"""

master_df3=master_df2.copy()

# 2,0,1 ARIMA Model

model = ARIMA(master_df['Weekly_Sales'], order=(2,0,1)) #p,d,q
results = model.fit()
results.summary()

"""(Coeff here refer to the weight of the term)

Observation:

1) The coefficients are not that close to zero.

2) The p-value are less than 0.05 except AR2, so we can consider this model.

3) The AIC value is also the least with these model parameter (2,0,1)

"""

residuals = pd.DataFrame(results.resid)
fig, ax = plt.subplots(1,2)
residuals.plot(figsize=(15,5),title="Residuals", ax=ax[0])
residuals.plot(figsize=(15,5),kind='kde', title='Density', ax=ax[1])
plt.show()

"""

Since the mean is almost at zero , theres is miniscule bias. But we can see some residual seasonality still present."""

results.plot_predict(dynamic=False)
plt.show()

fig = plt.figure(figsize=(20,8))
#num_points = len(clear_data['car.count'])
x = results.predict(start=("2012-4-01"), end=("2012-10-01"), dynamic=False)

plt.plot(train_data)
plt.plot(test_data)
plt.plot(x, color='g')
plt.figure(figsize=(12,5), dpi=100)

"""Although, its following the general trend of sales, there is a consntant deviation from the actual sales."""

print('Root Mean Squared Error (RMSE) of ARIMA: ', mean_squared_error(test_data, x)**(1/2))
print('Mean Absolute Deviation (MAD) of ARIMA: ', mean_absolute_error(test_data, x))

"""# Holt-Winter"""

# Fitting the Holt-Winters method for Weekly Sales.
from statsmodels.tsa.api import ExponentialSmoothing
model2 = ExponentialSmoothing(train_data, seasonal_periods=7, trend='additive', seasonal='add').fit() #Taking additive (played around with others) trend and seasonality.
print (model2.summary())

pred = model2.forecast(len(test_data))
plt.figure(figsize=(20,6))
plt.title('Prediction of Weekly Sales using Holt-Winters model', fontsize=20)
plt.plot(train_data, label='Train')
plt.plot(test_data, label='Test')
plt.plot(pred, label='Prediction using Holt Winters Methods')
plt.legend(loc='best')
plt.xlabel('Date', fontsize=14)
plt.ylabel('Weekly Sales', fontsize=14)
plt.show()

print('Root Mean Squared Error (RMSE) of Holt-Winters: ', mean_squared_error(test_data, pred)**(1/2))
print('Mean Absolute Deviation (MAD) of Holt-Winters: ', mean_absolute_error(test_data, pred))

"""
# Linear Regression"""

Dt = train['Weekly_Sales'] #DT-Dependant(y)
Idt = train.drop(['Weekly_Sales'], axis=1) #Idt-Independant(x)

Idt_train, Idt_test, Dt_train, Dt_test = train_test_split(Idt, Dt, test_size=0.3) # Train:Test = 70:30 splitting.

# Remove Date column as it does not allow the models to fit on the data.
Idt_train = Idt_train.drop(['Date'], axis=1)
Idt_test = Idt_test.drop(['Date'], axis=1)


def wmae_train(test, pred): # WMAE for train
  weights = Idt_train['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
  error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
  return error


def wmae_test(test, pred): # WMAE for test
  weights = Idt_test['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
  error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
  return error

#https://realpython.com/linear-regression-in-python/
model_linear_reg = LinearRegression()
model_linear_reg.fit(Idt_train,Dt_train) # Fit the model.

model_linear_reg=model_linear_reg.fit(Idt_train,Dt_train)

r_sq = model_linear_reg.score(Idt_train, Dt_train)
print('coefficient of determination:', r_sq)

"""Very low amount of variation in ùë¶ can be explained by the dependence on ùê± using this particular regression model as R is very low"""

print('intercept:', model_linear_reg.intercept_)
print('slope:', model_linear_reg.coef_)

"""Intercept is y when x=0 and slopes are calculated for each independant variable. Hence 15 values."""

y_pred = model_linear_reg.predict(Idt_test) # Predict test data.

print('Weighted Mean Absolute Error (WMAE) for Linear Regression:', wmae_test(Dt_test, y_pred)) # Calculate WMAE score.
print('Mean Squared Error (RMSE) of Linear Regression:', mean_squared_error(Dt_test, y_pred)**(1/2))

"""# Decision Tree Regression"""

# Define the list of errors and list of hyper parameters.
error_cv_dt = []
error_train_dt = []
max_depth = [1,5,10,15,20,25,30,35]
min_samples_leaf = [1,2,3,4,5,6,7,8]
dt_hyperparams = []

for i in max_depth: # Loop over max_depth.
    for j in min_samples_leaf: # Loop over min_samples_leaf.
        dt = DecisionTreeRegressor(max_depth=i, min_samples_leaf=j) # Apply Decision Tree Regressor.
        Z=dt.fit(X_train, y_train) # Fit the model.
        y_pred_cv_dt = dt.predict(X_cv) # Predict CV data.
        y_pred_train_dt = dt.predict(X_train) # Predict Train data.
        error_cv_dt.append(wmae_cv(y_cv, y_pred_cv_dt)) # Calculate CV error.
        error_train_dt.append(wmae_train(y_train, y_pred_train_dt)) # Calculate Train error.
        dt_hyperparams.append({'depth':i, 'leaf':j}) # Get the list of hyper parameters.

dt_dataframe = pd.DataFrame(dt_hyperparams)
dt_dataframe['train error'] = error_train_dt
dt_dataframe['cv error'] = error_cv_dt
dt_dataframe.sort_values(by=['cv error'], ascending=True)

sns.set(font_scale=1,)
train_dt = pd.pivot_table(dt_dataframe ,'train error','depth','leaf') # Pivot table for train.
cv_dt = pd.pivot_table(dt_dataframe , 'cv error','depth','leaf') # Pivot table for CV.
fig, ax = plt.subplots(1,2, figsize=(23,6))
sns.heatmap(train_dt, annot=True, fmt='.2f', ax=ax[0]) # Train heatmap.
sns.heatmap(cv_dt, annot=True, fmt='.2f', ax=ax[1]) # CV heatmap.
ax[0].set_title('Training set')
ax[1].set_title('CV set')
plt.show()

"""After much trial and error:
We can observe that training errors and CV errors are least for minimum sample leaf of 6 and almost similar from depth 25 to 35. Thus to prevent over-fitting, we use 25 as the depth of the tree.
"""

model_dt = DecisionTreeRegressor(max_depth=25, min_samples_leaf=6).fit(X_train, y_train) # Fit the model with best hyper parameter values.

y_pred = model_dt.predict(X_test) # Predict the test data.
model_dt

from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

# import export_graphviz
from sklearn.tree import export_graphviz
from graphviz import Source
from sklearn import tree

#graph = Source( tree.export_graphviz(model_dt, out_file=None, feature_names=X_train.columns))
#png_bytes = graph.pipe(format='png')
#with open('dtree_pipe.png','wb') as f:
 #   f.write(png_bytes)

#from IPython.display import Image
#Image(png_bytes)

print('Weighted Mean Absolute Error (WMAE) for Decision Tree Regression:', wmae_test(y_test, y_pred))
print('Mean Squared Error (RMSE) of Decision Tree Regression: ', mean_squared_error(y_test, y_pred)**(1/2))

graph = Source( tree.export_graphviz(model_dt, out_file=None, feature_names=X_train.columns, max_depth=3))
png_bytes = graph.pipe(format='png')
with open('decisiontree_pipe.png','wb') as f:
    f.write(png_bytes)

from IPython.display import Image
Image(png_bytes)

"""# Random Forest Regression"""

error_cv_rf = []
error_train_rf = []
max_depth = [1,5,10,15,20,25,30,35]
n_estimators = [10,20,30,40,50,60,70,80,90,100]
rf_hyperparams = []

for i in max_depth: # Loop over max_depth.
    for j in n_estimators: # Loop over n_estimators.
        rf = RandomForestRegressor(max_depth=i, n_estimators=j) # Apply Random Forest Regressor.
        rf.fit(X_train, y_train) # Fit the model.
        y_pred_cv_rf = rf.predict(X_cv) # Predict CV data.
        y_pred_train_rf = rf.predict(X_train) # Predict Train data.
        error_cv_rf.append(wmae_cv(y_cv, y_pred_cv_rf)) # Get CV error.
        error_train_rf.append(wmae_train(y_train, y_pred_train_rf)) # Get Train error.
        rf_hyperparams.append({'Maximum Depth':i, 'No. of Estimators':j}) # Get list of hyper parameter values.

rf_dataframe = pd.DataFrame(rf_hyperparams)
rf_dataframe['Train Error']=error_train_rf
rf_dataframe['CV Error']=error_cv_rf
rf_dataframe.sort_values(by=['CV Error'], ascending=True)

sns.set(font_scale=1.0)
train_rf = pd.pivot_table(rf_dataframe,'Train Error','Maximum Depth','No. of Estimators') # Pivot table of Train data.
cv_rf = pd.pivot_table(rf_dataframe, 'CV Error','Maximum Depth','No. of Estimators') # Pivot table of CV data.
fig, ax = plt.subplots(1,2, figsize=(25,6))
ax_train = sns.heatmap(train_rf, annot=True, fmt='2g', ax=ax[0], linewidths=0.01)
ax_cv = sns.heatmap(cv_rf, annot=True, fmt='4g', ax=ax[1], linewidths=0.01)

bottom_train, top_train = ax_train.get_ylim()
ax_train.set_ylim(bottom_train + 0.5, top_train - 0.5)

bottom_cv, top_cv = ax_cv.get_ylim()
ax_cv.set_ylim(bottom_cv + 0.5, top_cv - 0.5)

ax[0].set_title('Training set')
ax[1].set_title('CV set')
plt.show()

model_rf = RandomForestRegressor(max_depth= 35, n_estimators=90).fit(X_train, y_train) # Fit the model with best hyper parameter values.

y_pred = model_rf.predict(X_test) # Predict the test data.

r2_score(y_test,y_pred)

print('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', wmae_test(y_test, y_pred)) # Get WMAE score.
print('Mean Squared Error (RMSE) of Holt-Winters: ', mean_squared_error(y_test, y_pred)**(1/2))

model_rf = RandomForestRegressor(max_depth= 35, n_estimators=100).fit(X_train, y_train) # Fit the model with best hyper parameter values.
y_pred = model_rf.predict(X_test) # Predict the test data.
print('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', wmae_test(y_test, y_pred)) # Get WMAE score.

model_rf = RandomForestRegressor(max_depth= 30, n_estimators=90).fit(X_train, y_train) # Fit the model with best hyper parameter values.
y_pred = model_rf.predict(X_test) # Predict the test data.
print('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', wmae_test(y_test, y_pred)) # Get WMAE score.

model_rf = RandomForestRegressor(max_depth= 25, n_estimators=90).fit(X_train, y_train) # Fit the model with best hyper parameter values.
y_pred = model_rf.predict(X_test) # Predict the test data.
print('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', wmae_test(y_test, y_pred)) # Get WMAE score.

model_rf = RandomForestRegressor(max_depth= 25, n_estimators=100).fit(X_train, y_train) # Fit the model with best hyper parameter values.
y_pred = model_rf.predict(X_test) # Predict the test data.
print('Weighted Mean Absolute Error (WMAE) for Random Forest Regression:', wmae_test(y_test, y_pred)) # Get WMAE score.